---
title: "Untitled"
output: html_document
---
#https://www.shirin-glander.de/2019/01/text_classification_keras_data_prep/
#
---
title: "Neural Network 2"
output: html_document
---

```{r}
library(keras)
library(tidyverse)
library(pacman)
p_load(readr, tidyverse,rsample,recipes, textrecipes, parsnip, yardstick,workflows, discrim,kernlab)
p_load(tidyverse, stringr, tm, ggplot2, GGally, e1071, caret,stopwords, stringi, tm, SnowballC,stringr,fastmatch)

```

```{r}
loading_data <- function(path) {
  read_delim(path, "\t", escape_double = FALSE, trim_ws = TRUE)
}

training <- loading_data("offenseval-da-training-v1.tsv") %>% 
          mutate(Id = id,tag = factor(subtask_a),text=tweet) %>% 
          na.omit()

testing<-loading_data("offenseval-da-test-v1.tsv") %>% 
          mutate(Id = id,tag = factor(subtask_a),text=tweet) %>% 
          na.omit()


###CLEANING####
# Remove stopwords
testing$text<-tolower(testing$text) 
training$text<-tolower(training$text) 
stopwords_regex = paste(stopwords("da",source= "snowball"), collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')
training$text = stringr::str_replace_all(training$text, stopwords_regex, '')
# remove numbers
training$text <-  removeNumbers(training$text)
# Stem words
training$text <-  wordStem(training$text, language = "danish")
#repeat for test data
stopwords_regex = paste(stopwords("da",source= "snowball"), collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')
testing$text = stringr::str_replace_all(testing$text, stopwords_regex, '')
# remove numbers
testing$text <-  removeNumbers(testing$text)
# Stem words
testing$text <-  wordStem(testing$text, language = "danish")
# remove punctuation
testing$text<-removePunctuation(testing$text)
training$text<-removePunctuation(training$text)


training<-training[,4:6]
testing<-testing[,4:6]

training$tag<-as.numeric(training$tag)-1
testing$tag<-as.numeric(testing$tag)-1

glimpse(training)


###

```

```{r}
#train

text <- training$text

max_features <- 1000
tokenizer <- text_tokenizer(num_words = max_features)

#test

text_test <- testing$text

max_features_test <- 1000
tokenizer_test <- text_tokenizer(num_words = max_features)
```

```{r}
#Train
tokenizer %>% 
  fit_text_tokenizer(text)

#Test
tokenizer_test %>% 
  fit_text_tokenizer(text_test)
```

```{r}
tokenizer$document_count
tokenizer_test$document_count

```

```{r}
tokenizer$word_index %>%
  head()

tokenizer_test$word_index %>%
  head()

```

```{r}
text_seqs <- texts_to_sequences(tokenizer, text)

text_seqs %>%
  head()

#Test
text_seqs_test <- texts_to_sequences(tokenizer_test, text_test)

text_seqs_test %>%
  head()
```


```{r}
##
maxlen <- 100
batch_size <- 32
embedding_dims <- 50
filters <- 64
kernel_size <- 3
hidden_dims <- 50
epochs <- 5

```

```{r}
x_train <- text_seqs %>%
  pad_sequences(maxlen = maxlen)
dim(x_train)

#Test
x_test <- text_seqs_test %>%
  pad_sequences(maxlen = maxlen)
dim(x_test)
```

```{r}
y_train <- training$tag
length(y_train)

#Test
y_test <- testing$tag
length(y_test)

y_test<-y_test
y_train<-y_train
```

```{r}
model <- keras_model_sequential()
model %>%
  layer_embedding(input_dim = max_features, output_dim = 128) %>% 
  layer_lstm(units = 64, dropout = 0.2, recurrent_dropout = 0.2) %>% 
  layer_dense(units = 1, activation = 'sigmoid')

```

```{r}
# Try using different optimizers and different optimizer configs
model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = 'adam',
  metrics = c('accuracy')
)
```

```{r}

model %>% fit(
  x_train, y_train,
  batch_size = batch_size,
  epochs = 15,
  validation_data = list(x_test, y_test)

)
```

```{r}
scores <- model %>% tensorflow::evaluate(
  x_test, y_test,
  batch_size = batch_size
)

?evaluate


y_test

testing$predict<-predict_classes(model, testing$tag)
?predict_classes



testing$predict<-predict_classes(model, x_test) #Predict class
Raw_probs<-predict_proba(model, x_test) #Predict raw probabilites (prob of tweet = OFF)

#testing<-testing[,1:3]
```

#CNN:

```{r}
# Embedding
max_features = 20000
maxlen = 100
embedding_size = 128

```

```{r}
# Convolution
kernel_size = 5
filters = 64
pool_size = 4
```

```{r}
# LSTM
lstm_output_size = 70

```

```{r}
# Training
batch_size = 30
epochs = 15
```

```{r}
# Defining Model ------------------------------------------------------

model <- keras_model_sequential()

model %>%
  layer_embedding(max_features, embedding_size, input_length = maxlen) %>%
  layer_dropout(0.25) %>%
  layer_conv_1d(
    filters, 
    kernel_size, 
    padding = "valid",
    activation = "relu",
    strides = 1
  ) %>%
  layer_max_pooling_1d(pool_size) %>%
  layer_lstm(lstm_output_size) %>%
  layer_dense(1) %>%
  layer_activation("sigmoid")

model %>% compile(
  loss = "binary_crossentropy",
  optimizer = "adam",
  metrics = c("accuracy","Precision","Recall")
)
```

```{r}
model %>% fit(
  x_train, y_train,
  batch_size = batch_size,
  epochs = epochs,
  validation_data = list(x_test, y_test)
)

scores <- model %>% tensorflow::evaluate(
  x_test, y_test,
  batch_size = batch_size
)

testing$predict<-predict_classes(model, x_test) #Predict class
Raw_probs<-predict_proba(model, x_test) #Predict raw probabilites (prob of tweet = OFF)



```

